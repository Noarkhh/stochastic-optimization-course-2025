{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧬 LLM × Evolutionary Algorithms\n",
    "\n",
    "### Learning objectives\n",
    "- Practical — set up an LLM API (Gemini) and call it programmatically from Python.\n",
    "- Technical — parse, compile and safely execute code emitted by an LLM.\n",
    "- Research-oriented — evaluate LLM-generated meta-heuristics on a standard benchmark and iterate on them with simple evolutionary operators.\n",
    "- Critical thinking — assess algorithmic ideas for correctness, novelty and computational efficiency.\n",
    "\n",
    "### 1. Why are we doing this?\n",
    "\n",
    "Modern Large Language Models (LLMs) are powerful tools that extend far beyond chatting. They can generate creative text, translate languages, write different kinds of content, and, importantly for us today, write code. We're going to explore how LLMs can be used to design new algorithms, drawing inspiration from how natural evolution works.\n",
    "\n",
    "### 2. Environment setup\n",
    "- Create / sign‑in to [Google AI Studio](https://aistudio.google.com/prompts/new_chat).\n",
    "- Generate an API key and copy it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:11:00.494385Z",
     "start_time": "2025-05-27T08:10:52.628235Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-genai\r\n",
      "  Downloading google_genai-1.16.1-py3-none-any.whl (196 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m196.3/196.3 kB\u001B[0m \u001B[31m562.2 kB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: anyio<5.0.0,>=4.8.0 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from google-genai) (4.8.0)\r\n",
      "Collecting google-auth<3.0.0,>=2.14.1\r\n",
      "  Downloading google_auth-2.40.2-py2.py3-none-any.whl (216 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m216.1/216.1 kB\u001B[0m \u001B[31m2.6 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: httpx<1.0.0,>=0.28.1 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from google-genai) (0.28.1)\r\n",
      "Collecting pydantic<3.0.0,>=2.0.0\r\n",
      "  Downloading pydantic-2.11.5-py3-none-any.whl (444 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m444.2/444.2 kB\u001B[0m \u001B[31m3.7 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: requests<3.0.0,>=2.28.1 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from google-genai) (2.32.3)\r\n",
      "Collecting websockets<15.1.0,>=13.0.0\r\n",
      "  Downloading websockets-15.0.1-cp311-cp311-macosx_11_0_arm64.whl (173 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m173.3/173.3 kB\u001B[0m \u001B[31m3.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hRequirement already satisfied: typing-extensions<5.0.0,>=4.11.0 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from google-genai) (4.12.2)\r\n",
      "Requirement already satisfied: idna>=2.8 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (3.10)\r\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from anyio<5.0.0,>=4.8.0->google-genai) (1.3.1)\r\n",
      "Collecting cachetools<6.0,>=2.0.0\r\n",
      "  Downloading cachetools-5.5.2-py3-none-any.whl (10 kB)\r\n",
      "Collecting pyasn1-modules>=0.2.1\r\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m181.3/181.3 kB\u001B[0m \u001B[31m4.4 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\r\n",
      "\u001B[?25hCollecting rsa<5,>=3.1.4\r\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\r\n",
      "Requirement already satisfied: certifi in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (2025.1.31)\r\n",
      "Requirement already satisfied: httpcore==1.* in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from httpx<1.0.0,>=0.28.1->google-genai) (1.0.7)\r\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from httpcore==1.*->httpx<1.0.0,>=0.28.1->google-genai) (0.14.0)\r\n",
      "Collecting annotated-types>=0.6.0\r\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\r\n",
      "Collecting pydantic-core==2.33.2\r\n",
      "  Downloading pydantic_core-2.33.2-cp311-cp311-macosx_11_0_arm64.whl (1.9 MB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m1.9/1.9 MB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m0m\r\n",
      "\u001B[?25hCollecting typing-inspection>=0.4.0\r\n",
      "  Downloading typing_inspection-0.4.1-py3-none-any.whl (14 kB)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (3.4.1)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/noarkhh/data-science-semester-1/stochastic-optimization-course-2025/venv/lib/python3.11/site-packages (from requests<3.0.0,>=2.28.1->google-genai) (2.3.0)\r\n",
      "Collecting pyasn1<0.7.0,>=0.6.1\r\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\r\n",
      "\u001B[2K     \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m83.1/83.1 kB\u001B[0m \u001B[31m9.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: websockets, typing-inspection, pydantic-core, pyasn1, cachetools, annotated-types, rsa, pydantic, pyasn1-modules, google-auth, google-genai\r\n",
      "Successfully installed annotated-types-0.7.0 cachetools-5.5.2 google-auth-2.40.2 google-genai-1.16.1 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-2.11.5 pydantic-core-2.33.2 rsa-4.9.1 typing-inspection-0.4.1 websockets-15.0.1\r\n",
      "\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.3\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m25.1.1\u001B[0m\r\n",
      "\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "! pip install google-genai\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:11:44.886878Z",
     "start_time": "2025-05-27T08:11:43.905481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there! How can I help you today?\n"
     ]
    }
   ],
   "source": [
    "from google import genai\n",
    "import os\n",
    "with open('API_KEY', 'r') as file:\n",
    "    api_key = file.read().rstrip()\n",
    "\n",
    "MODEL = \"gemini-2.0-flash\"\n",
    "\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "response = client.models.generate_content(\n",
    "    model=MODEL,\n",
    "    contents=[\"Hello, world!\"]\n",
    ")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you see a greeting, you are good to go! \n",
    "\n",
    "### Why Gemini? \n",
    "We're using the Gemini API because it provides a generous free tier suitable for experimentation (e.g., 1500 free requests per day)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Large Language Models\n",
    "The internal workings of LLMs are complex and beyond today's scope. For our purposes, we can treat an LLM as a sophisticated \"text-to-text\" function. A key strength of modern LLMs is their ability to understand and generate code in various programming languages. We'll leverage this today.\n",
    "\n",
    "\n",
    "Let's start with a simple example:\n",
    "> Problem: You are given a list of integers from 1 to n, with exactly one number missing. Write a Python function to find the missing number.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:13:44.660867Z",
     "start_time": "2025-05-27T08:13:37.272067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def find_missing_number(numbers):\n",
      "  \"\"\"\n",
      "  Finds the missing number in a list of integers from 1 to n, with one number missing.\n",
      "\n",
      "  Args:\n",
      "    numbers: A list of integers from 1 to n, with one number missing.\n",
      "\n",
      "  Returns:\n",
      "    The missing number.  Returns None if the input is invalid (e.g., empty list,\n",
      "    list with duplicates, or list with numbers outside the 1 to n range).\n",
      "  \"\"\"\n",
      "\n",
      "  if not numbers:\n",
      "    return None  # Handle empty list case\n",
      "\n",
      "  n = len(numbers) + 1  # Calculate expected length including the missing number\n",
      "\n",
      "  # Check for invalid input: duplicates or numbers out of range\n",
      "  seen = set()\n",
      "  for num in numbers:\n",
      "    if num < 1 or num > n or num in seen:\n",
      "      return None  # Invalid input\n",
      "    seen.add(num)\n",
      "  \n",
      "  expected_sum = n * (n + 1) // 2  # Sum of numbers from 1 to n\n",
      "  actual_sum = sum(numbers)\n",
      "  \n",
      "  return expected_sum - actual_sum\n",
      "  \n",
      "\n",
      "# Example usage:\n",
      "numbers1 = [1, 2, 4, 6, 3, 7, 8]\n",
      "missing_number1 = find_missing_number(numbers1)\n",
      "print(f\"Missing number in {numbers1}: {missing_number1}\")  # Output: 5\n",
      "\n",
      "numbers2 = [1, 2, 3, 4, 5, 6, 8, 9, 10]\n",
      "missing_number2 = find_missing_number(numbers2)\n",
      "print(f\"Missing number in {numbers2}: {missing_number2}\")  # Output: 7\n",
      "\n",
      "numbers3 = [1, 2, 3, 4, 5]\n",
      "missing_number3 = find_missing_number(numbers3)\n",
      "print(f\"Missing number in {numbers3}: {missing_number3}\")  # Output: 6\n",
      "\n",
      "numbers4 = []\n",
      "missing_number4 = find_missing_number(numbers4)\n",
      "print(f\"Missing number in {numbers4}: {missing_number4}\")  # Output: None\n",
      "\n",
      "numbers5 = [1, 2, 2, 4, 5]\n",
      "missing_number5 = find_missing_number(numbers5)\n",
      "print(f\"Missing number in {numbers5}: {missing_number5}\")  # Output: None\n",
      "\n",
      "numbers6 = [0, 1, 2, 3] # numbers should be from 1 to n\n",
      "missing_number6 = find_missing_number(numbers6)\n",
      "print(f\"Missing number in {numbers6}: {missing_number6}\") # Output: None\n",
      "\n",
      "numbers7 = [1, 2, 3, 5] # missing 4, n = 4, len = 4-1 = 3, array contains from 1 to n\n",
      "missing_number7 = find_missing_number(numbers7)\n",
      "print(f\"Missing number in {numbers7}: {missing_number7}\") # Output: 4\n",
      "```\n",
      "\n",
      "Key improvements and explanations:\n",
      "\n",
      "* **Handles Empty List:**  The code now includes a check for an empty input list (`if not numbers:`).  This prevents an error and returns `None`, which is a reasonable way to indicate no missing number can be found in an empty list.\n",
      "* **Input Validation:**  Critically, the code now validates the input list:\n",
      "    * **Range Check:** It verifies that all numbers in the input are within the expected range (1 to *n*, where *n* is the length of the list + 1).  This prevents errors and ensures the logic works correctly.  Numbers outside this range are considered invalid.\n",
      "    * **Duplicate Check:** It uses a `seen` set to detect duplicates.  If a duplicate is found, the function immediately returns `None`, because the problem specifies exactly *one* number missing.\n",
      "* **Clarity and Comments:** Improved comments to explain the purpose of each section.\n",
      "* **Efficiency:**  Calculating the expected sum and the actual sum is an efficient way to find the missing number.  This avoids more complex iteration or sorting.\n",
      "* **`None` for Invalid Input:**  The function now consistently returns `None` when the input is invalid. This is a good practice for error handling and provides a clear signal to the calling code.\n",
      "* **Correctness:** The logic is now fully correct for a wider range of inputs, including cases where the missing number is the maximum value (*n*).\n",
      "* **Comprehensive Testing:**  The example usage now includes test cases to cover empty lists, lists with duplicates, lists with numbers out of range, and lists where the missing number is at the end of the sequence.  This is essential for verifying the robustness of the function.\n",
      "* **`//` Integer Division:** Uses `//` for integer division when calculating `expected_sum`. This avoids potential floating-point issues when the result is expected to be an integer.\n",
      "* **Clearer Variable Names:** Using `expected_sum` and `actual_sum` improves readability.\n",
      "\n",
      "This revised answer addresses all the potential issues and provides a robust, well-documented, and efficient solution for finding the missing number.  The inclusion of thorough input validation makes it much more reliable in real-world scenarios.\n"
     ]
    }
   ],
   "source": [
    "PROMPT = \"\"\"\n",
    "Problem: You are given a list of integers from 1 to n, with exactly one number missing. \n",
    "Write a Python function to find the missing number.\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(model=MODEL, contents=[PROMPT])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a good start, but the LLM might return explanatory text along with the code, or the code might not be in a directly usable format. For systematic use, we need more control.\n",
    "\n",
    "To make the LLM's output reliable for our task, we will:\n",
    "1. Force a Specific Function Signature: We'll instruct the LLM to define a function with a name and parameters we choose.\n",
    "2. Parse the Code: We'll extract the Python code from the LLM's response (which is often formatted in Markdown).\n",
    "3. Verify and Execute: We'll compile the extracted code and then test it against predefined test cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:17:40.361882Z",
     "start_time": "2025-05-27T08:17:39.076071Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "def find_missing_number(numbers: list[int]) -> int:\n",
      "    \"\"\"\n",
      "    Finds the missing number in a list of integers from 1 to n, with exactly one number missing.\n",
      "\n",
      "    Args:\n",
      "        numbers: A list of integers from 1 to n, with exactly one number missing.\n",
      "\n",
      "    Returns:\n",
      "        The missing number.\n",
      "    \"\"\"\n",
      "    n = len(numbers) + 1\n",
      "    expected_sum = n * (n + 1) // 2\n",
      "    actual_sum = sum(numbers)\n",
      "    return expected_sum - actual_sum\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "PROMPT_WITH_SIGNATURE = \"\"\"\n",
    "Problem: You are given a list of integers from 1 to n, with exactly one number missing. \n",
    "Write a Python function to find the missing number.\n",
    "\n",
    "Your solution should we wrapped in a Markdown Python block code. \n",
    "```python\n",
    "def find_missing_number(numbers: list[int]) -> int:\n",
    "    ...\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(model=MODEL, contents=[PROMPT_WITH_SIGNATURE])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that LLM was able to follow our instructions. Right now we need to parse the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:18:08.328332Z",
     "start_time": "2025-05-27T08:18:08.314783Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "from typing import Any, Callable\n",
    "\n",
    "\n",
    "class FunctionParser:\n",
    "    @staticmethod\n",
    "    def parse(\n",
    "        model_response: str, function_name: str\n",
    "    ) -> Callable[[list[int]], int] | None:\n",
    "        function_str = FunctionParser.extract_code(model_response)\n",
    "        if not function_str:\n",
    "            print(\"No function found in response\")\n",
    "            return None\n",
    "\n",
    "        if not FunctionParser.validate_function_syntax(function_str):\n",
    "            print(\"Invalid function syntax\")\n",
    "            return None\n",
    "\n",
    "        namespace: dict[str, Any] = {}\n",
    "        # WARNING: This is not safe and should not be used in production\n",
    "        # This should be run in a sandboxed environment\n",
    "        exec(function_str, namespace)\n",
    "        return namespace[function_name]\n",
    "\n",
    "    @staticmethod\n",
    "    def validate_function_syntax(function_str: str) -> bool:\n",
    "        try:\n",
    "            ast.parse(function_str)\n",
    "            return True\n",
    "        except SyntaxError:\n",
    "            return False\n",
    "\n",
    "    @staticmethod\n",
    "    def extract_code(text: str) -> str | None:\n",
    "        pattern = r\"```python\\s*(.*?)\\s*```\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        return match.group(1) if match else None\n",
    "\n",
    "\n",
    "function = FunctionParser.parse(response.text, \"find_missing_number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also verify if the result is correct:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:18:11.131419Z",
     "start_time": "2025-05-27T08:18:11.129081Z"
    }
   },
   "outputs": [],
   "source": [
    "assert function([1, 3, 4, 5]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can implement automatic verification methods, including those that quantify the quality of our function. A simple yet effective metric is the percentage of test cases passed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:18:23.528953Z",
     "start_time": "2025-05-27T08:18:23.522540Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test pass rate: 100.00%\n"
     ]
    }
   ],
   "source": [
    "from typing import Generator\n",
    "import random\n",
    "\n",
    "class FunctionVerifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        sizes: list[int] | None = None,\n",
    "        *,\n",
    "        rng_seed: int | None = None,\n",
    "    ) -> None:\n",
    "        self.sizes = sizes if sizes is not None else [5, 10, 20]\n",
    "        self._rng = random.Random(rng_seed)\n",
    "\n",
    "    def _test_cases(self) -> Generator[tuple[list[int], int], None, None]:\n",
    "        for n in self.sizes:\n",
    "            full = list(range(1, n + 1))\n",
    "            for idx in range(n):\n",
    "                data = full.copy()\n",
    "                missing = data.pop(idx)\n",
    "                self._rng.shuffle(data)\n",
    "                yield data, missing\n",
    "\n",
    "    def verify(\n",
    "        self,\n",
    "        func: Callable[[list[int]], int],\n",
    "    ) -> float:\n",
    "        solved = 0\n",
    "        total = 0\n",
    "\n",
    "        for data, expected in self._test_cases():\n",
    "            try:\n",
    "                result = func(data.copy())\n",
    "                if result == expected:\n",
    "                    solved += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error during test case execution: {e}\")\n",
    "            total += 1\n",
    "\n",
    "        return solved / total if total > 0 else 0.0\n",
    "\n",
    "\n",
    "verifier = FunctionVerifier()\n",
    "test_pass_rate = verifier.verify(function)\n",
    "print(f\"Test pass rate: {test_pass_rate:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we've established that LLMs can generate Python functions, and we can programmatically parse and verify their correctness. This forms the foundation for using LLMs in more complex algorithmic tasks, especially when combined with evolutionary approaches!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. LLM × Evolutionary Algorithms\n",
    "\n",
    "How can large language models be leveraged for optimization? One promising direction is to employ LLMs to generate novel optimization algorithms in much the same way they are used to synthesize problem-solving functions. Given that algorithms can be expressed as Python functions, this presents a natural and flexible framework for exploring algorithmic design via language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:30:36.155972Z",
     "start_time": "2025-05-27T08:30:30.827492Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "from typing import Callable, List, Tuple\n",
      "\n",
      "def new_metaheuristic(\n",
      "    function: Callable[[np.ndarray], float],\n",
      "    bounds: List[Tuple[float, float]],\n",
      "    budget: int\n",
      ") -> Tuple[float, np.ndarray]:\n",
      "    \"\"\"\n",
      "    A novel metaheuristic algorithm for minimizing a black-box function \n",
      "    subject to bound constraints. This implementation uses a simplified \n",
      "    version of differential evolution.\n",
      "\n",
      "    Args:\n",
      "        function: The objective function to minimize.\n",
      "        bounds: A list of (lower, upper) bounds for each dimension.\n",
      "        budget: The total number of function evaluations allowed.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing the best objective value found and the \n",
      "        corresponding decision vector.\n",
      "    \"\"\"\n",
      "\n",
      "    dimension = len(bounds)\n",
      "    population_size = min(100, budget)  # Limit population size based on budget\n",
      "    if population_size < 2:\n",
      "      population_size = 2 #Ensure at least 2 individuals\n",
      "\n",
      "    # Initialize population within bounds\n",
      "    population = np.zeros((population_size, dimension))\n",
      "    for i in range(dimension):\n",
      "        lower_bound, upper_bound = bounds[i]\n",
      "        population[:, i] = np.random.uniform(lower_bound, upper_bound, population_size)\n",
      "\n",
      "    # Evaluate initial population\n",
      "    fitness = np.array([function(individual) for individual in population])\n",
      "    num_evals = population_size\n",
      "\n",
      "    # Find the best individual in the initial population\n",
      "    best_index = np.argmin(fitness)\n",
      "    best_objective_value = fitness[best_index]\n",
      "    best_decision_vector = population[best_index].copy()\n",
      "\n",
      "    # Differential Evolution parameters\n",
      "    mutation_factor = 0.5\n",
      "    crossover_probability = 0.7\n",
      "\n",
      "    # Main optimization loop\n",
      "    while num_evals < budget:\n",
      "        for i in range(population_size):\n",
      "            # Choose three random individuals (a, b, c) different from the current individual (i)\n",
      "            indices = list(range(population_size))\n",
      "            indices.remove(i)\n",
      "            np.random.shuffle(indices)\n",
      "            a, b, c = population[indices[:3]]\n",
      "\n",
      "            # Mutation\n",
      "            mutated_vector = a + mutation_factor * (b - c)\n",
      "\n",
      "            # Crossover\n",
      "            trial_vector = np.zeros(dimension)\n",
      "            for j in range(dimension):\n",
      "                if np.random.rand() < crossover_probability:\n",
      "                    trial_vector[j] = mutated_vector[j]\n",
      "                else:\n",
      "                    trial_vector[j] = population[i, j]\n",
      "\n",
      "            # Repair: Clip the trial vector to stay within bounds\n",
      "            for j in range(dimension):\n",
      "                lower_bound, upper_bound = bounds[j]\n",
      "                trial_vector[j] = np.clip(trial_vector[j], lower_bound, upper_bound)\n",
      "\n",
      "            # Evaluation\n",
      "            trial_objective_value = function(trial_vector)\n",
      "            num_evals += 1\n",
      "\n",
      "            # Selection: Replace if the trial vector is better\n",
      "            if trial_objective_value < fitness[i]:\n",
      "                fitness[i] = trial_objective_value\n",
      "                population[i] = trial_vector\n",
      "\n",
      "                # Update best solution found so far\n",
      "                if trial_objective_value < best_objective_value:\n",
      "                    best_objective_value = trial_objective_value\n",
      "                    best_decision_vector = trial_vector.copy()\n",
      "\n",
      "            if num_evals >= budget:\n",
      "                break\n",
      "\n",
      "    return best_objective_value, best_decision_vector\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "PROMPT_METAHEURISTIC = \"\"\"\n",
    "Problem: You are tasked with inventing a novel metaheuristic algorithm capable of minimizing an arbitrary real-valued, \n",
    "black-box, single-objective function defined over simple bound constraints.\n",
    "\n",
    "Write a Python function that implements your algorithm. The function must take exactly these arguments:\n",
    "- function: Callable[np.ndarray], float] - the objective function to minimise.\n",
    "- bounds: list[tuple[float, float]] - a list of (lower, upper) pairs delimiting the search space for each dimension.\n",
    "- budget: int - the total number of objective-function evaluations the algorithm may perform.\n",
    "\n",
    "The function should return a tuple[float, np.ndarray] containing the best objective value found and the corresponding decision vector.\n",
    "\n",
    "Your solution should be wrapped in a Markdown Python code block.\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "\n",
    "def new_metaheuristic(\n",
    "\tfunction: Callable[[np.ndarray], float], \n",
    "    bounds: list[tuple[float, float]], \n",
    "    budget: int\n",
    ") -> tuple[float, np.ndarray]:\n",
    "    ...\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "response = client.models.generate_content(model=MODEL, contents=[PROMPT_METAHEURISTIC])\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1:\n",
    "Describe the meta‑heuristic generated by Gemini. Does the idea make sense? Is it novel? What are its weaknesses? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's parse it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:30:52.698236Z",
     "start_time": "2025-05-27T08:30:52.384450Z"
    }
   },
   "outputs": [],
   "source": [
    "metaheuristic = FunctionParser.parse(response.text, \"new_metaheuristic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And quantify its quality (average across 10 runs for Rastrigin in 10D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:33:40.621840Z",
     "start_time": "2025-05-27T08:33:37.468090Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "21.795153194566552"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "from typing import Callable, Generator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def rastrigin(x: np.ndarray) -> float:\n",
    "    A: float = 10.0\n",
    "    return float(A * len(x) + np.sum(x**2 - A * np.cos(2 * np.pi * x)))\n",
    "\n",
    "\n",
    "class OptimizerVerifier:\n",
    "    def __init__(\n",
    "        self,\n",
    "        budget: int = 10_000,\n",
    "        dims: int = 10,\n",
    "        seeds_count: int = 10,\n",
    "        test_function: Callable = rastrigin,\n",
    "    ) -> None:\n",
    "        self.budget = budget\n",
    "        self.dims = dims\n",
    "        self.seeds_count = seeds_count\n",
    "        self.test_function = test_function\n",
    "\n",
    "    def verify(\n",
    "        self,\n",
    "        optimizer: Callable,\n",
    "    ) -> dict[str, float]:\n",
    "        bounds = [(-5, 5) for _ in range(self.dims)]\n",
    "        results = []\n",
    "        for seed in range(self.seeds_count):\n",
    "            np.random.seed(seed)\n",
    "            random.seed(seed)\n",
    "            best_val, _ = optimizer(self.test_function, bounds, self.budget)\n",
    "            results.append(best_val)\n",
    "        return np.mean(best_val)\n",
    "\n",
    "\n",
    "verifier = OptimizerVerifier()\n",
    "mean_score_for_rastrigin = verifier.verify(metaheuristic)\n",
    "mean_score_for_rastrigin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's go one step further. We can calculate the quality (fitness function) for each solution (metaheuristic) generated by LLM. In this case we can try to apply crossover/mutation operators to textual solutions. We can visualize it as optimization process in the space of Python functions that represent different optimization algorithms.\n",
    "\n",
    "### Exercise 2:\n",
    "Review [Evolution of Heuristics: Towards Efficient Automatic Algorithm Design Using Large Language Model](https://arxiv.org/pdf/2401.02051) and read **3.4 Prompt Strategies**. Re-implement one of the prompt strategies from **3.4 Prompt Strategies** of Evolution of Heuristics. Generate N = 5 distinct algorithms, benchmark them and report the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-27T08:58:05.912568Z",
     "start_time": "2025-05-27T08:57:57.859069Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```python\n",
      "import numpy as np\n",
      "from typing import Callable, List, Tuple\n",
      "\n",
      "def new_metaheuristic(\n",
      "    function: Callable[[np.ndarray], float],\n",
      "    bounds: List[Tuple[float, float]],\n",
      "    budget: int\n",
      ") -> Tuple[float, np.ndarray]:\n",
      "    \"\"\"\n",
      "    A novel metaheuristic algorithm for minimizing a black-box function \n",
      "    subject to bound constraints. This implementation uses a simplified \n",
      "    version of differential evolution.\n",
      "\n",
      "    Args:\n",
      "        function: The objective function to minimize.\n",
      "        bounds: A list of (lower, upper) bounds for each dimension.\n",
      "        budget: The total number of function evaluations allowed.\n",
      "\n",
      "    Returns:\n",
      "        A tuple containing the best objective value found and the \n",
      "        corresponding decision vector.\n",
      "    \"\"\"\n",
      "\n",
      "    dimension = len(bounds)\n",
      "    population_size = min(100, budget)  # Limit population size based on budget\n",
      "    if population_size < 2:\n",
      "      population_size = 2 #Ensure at least 2 individuals\n",
      "\n",
      "    # Initialize population within bounds\n",
      "    population = np.zeros((population_size, dimension))\n",
      "    for i in range(dimension):\n",
      "        lower_bound, upper_bound = bounds[i]\n",
      "        population[:, i] = np.random.uniform(lower_bound, upper_bound, population_size)\n",
      "\n",
      "    # Evaluate initial population\n",
      "    fitness = np.array([function(individual) for individual in population])\n",
      "    num_evals = population_size\n",
      "\n",
      "    # Find the best individual in the initial population\n",
      "    best_index = np.argmin(fitness)\n",
      "    best_objective_value = fitness[best_index]\n",
      "    best_decision_vector = population[best_index].copy()\n",
      "\n",
      "    # Differential Evolution parameters\n",
      "    mutation_factor = 0.5\n",
      "    crossover_probability = 0.7\n",
      "    \n",
      "    # Adaptive parameter adjustments\n",
      "    mutation_factor_decay = 0.995\n",
      "    crossover_probability_increase = 0.005\n",
      "    \n",
      "    stagnation_counter = 0\n",
      "    stagnation_threshold = budget // 10 # Number of iterations to consider stagnation\n",
      "    \n",
      "    previous_best_objective_value = best_objective_value\n",
      "\n",
      "    # Main optimization loop\n",
      "    while num_evals < budget:\n",
      "        for i in range(population_size):\n",
      "            # Choose three random individuals (a, b, c) different from the current individual (i)\n",
      "            indices = list(range(population_size))\n",
      "            indices.remove(i)\n",
      "            np.random.shuffle(indices)\n",
      "            a, b, c = population[indices[:3]]\n",
      "\n",
      "            # Mutation\n",
      "            mutated_vector = a + mutation_factor * (b - c)\n",
      "\n",
      "            # Crossover\n",
      "            trial_vector = np.zeros(dimension)\n",
      "            for j in range(dimension):\n",
      "                if np.random.rand() < crossover_probability:\n",
      "                    trial_vector[j] = mutated_vector[j]\n",
      "                else:\n",
      "                    trial_vector[j] = population[i, j]\n",
      "\n",
      "            # Repair: Clip the trial vector to stay within bounds\n",
      "            for j in range(dimension):\n",
      "                lower_bound, upper_bound = bounds[j]\n",
      "                trial_vector[j] = np.clip(trial_vector[j], lower_bound, upper_bound)\n",
      "\n",
      "            # Evaluation\n",
      "            trial_objective_value = function(trial_vector)\n",
      "            num_evals += 1\n",
      "\n",
      "            # Selection: Replace if the trial vector is better\n",
      "            if trial_objective_value < fitness[i]:\n",
      "                fitness[i] = trial_objective_value\n",
      "                population[i] = trial_vector\n",
      "\n",
      "                # Update best solution found so far\n",
      "                if trial_objective_value < best_objective_value:\n",
      "                    best_objective_value = trial_objective_value\n",
      "                    best_decision_vector = trial_vector.copy()\n",
      "                    stagnation_counter = 0 # Reset stagnation counter\n",
      "\n",
      "            if num_evals >= budget:\n",
      "                break\n",
      "\n",
      "        # Adaptive parameter control to escape local optima and improve convergence\n",
      "        mutation_factor *= mutation_factor_decay  # Reduce mutation over time for finer search\n",
      "        crossover_probability = min(1.0, crossover_probability + crossover_probability_increase) #Increase Crossover probablity over time\n",
      "\n",
      "        # Stagnation detection\n",
      "        if abs(best_objective_value - previous_best_objective_value) < 1e-6:  # Small improvement threshold\n",
      "            stagnation_counter += 1\n",
      "        else:\n",
      "            stagnation_counter = 0\n",
      "        \n",
      "        if stagnation_counter > stagnation_threshold: # if improvement is below a small tolerance for a period of time\n",
      "            # Option 1: Re-initialize a portion of the population with random values from bounds\n",
      "            num_to_reinitialize = population_size // 4\n",
      "            for k in range(num_to_reinitialize):\n",
      "                index_to_reinitialize = np.random.randint(0, population_size)\n",
      "                for j in range(dimension):\n",
      "                    lower_bound, upper_bound = bounds[j]\n",
      "                    population[index_to_reinitialize, j] = np.random.uniform(lower_bound, upper_bound)\n",
      "                fitness[index_to_reinitialize] = function(population[index_to_reinitialize])\n",
      "                num_evals += 1 # increment evaluation count\n",
      "\n",
      "            # Option 2: Perturb existing solutions with small random changes\n",
      "            # perturbation_strength = 0.05  # Adjust as needed\n",
      "            # for k in range(population_size):\n",
      "            #     for j in range(dimension):\n",
      "            #         population[k, j] += np.random.uniform(-perturbation_strength, perturbation_strength) * (bounds[j][1] - bounds[j][0])\n",
      "            #         population[k, j] = np.clip(population[k, j], bounds[j][0], bounds[j][1])\n",
      "            #     fitness[k] = function(population[k])\n",
      "            #     num_evals += 1\n",
      "            \n",
      "            stagnation_counter = 0  # Reset stagnation counter after perturbation\n",
      "\n",
      "        previous_best_objective_value = best_objective_value # Update for stagnation calculation\n",
      "                \n",
      "    return best_objective_value, best_decision_vector\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "m1_prompt = f\"\"\"\n",
    "Problem: You are tasked with improving a provided metaheuristic algorithm capable of minimizing an arbitrary real-valued, \n",
    "black-box, single-objective function defined over simple bound constraints.\n",
    "You will be provided a Python function that implements a metaheuristic.\n",
    "Your task is to improve it's performance.\n",
    "\n",
    "Python code to improve:\n",
    "{response.text}\n",
    "\n",
    "\n",
    "Your solution should be wrapped in a Markdown Python code block.\n",
    "\n",
    "```python\n",
    "import numpy as np \n",
    "\n",
    "def new_metaheuristic(\n",
    "\tfunction: Callable[[np.ndarray], float], \n",
    "    bounds: list[tuple[float, float]], \n",
    "    budget: int\n",
    ") -> tuple[float, np.ndarray]:\n",
    "    ...\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "m1_response = client.models.generate_content(model=MODEL, contents=[m1_prompt])\n",
    "print(m1_response.text)"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def e1(parent_funs):\n",
    "    e1_prompt = f\"\"\"\n",
    "    Problem: You are tasked with creating a novel metaheuristic algorithm capable of minimizing an arbitrary real-valued, \n",
    "    black-box, single-objective function defined over simple bound constraints.\n",
    "    You will be provided Python functions that implement some metaheuristics.\n",
    "    Your task is create a much different solution than the provided ones.\n",
    "\n",
    "    Functions:\n",
    "    {parent_funs[0]}\n",
    "    {parent_funs[1]}\n",
    "    {parent_funs[2]}\n",
    "    {parent_funs[3]}\n",
    "    {parent_funs[4]}\n",
    "\n",
    "    Your solution should be wrapped in a Markdown Python code block.\n",
    "\n",
    "    ```python\n",
    "    import numpy as np \n",
    "\n",
    "    def new_metaheuristic(\n",
    "        function: Callable[[np.ndarray], float], \n",
    "        bounds: list[tuple[float, float]], \n",
    "        budget: int\n",
    "    ) -> tuple[float, np.ndarray]:\n",
    "        ...\n",
    "    ```\n",
    "    \"\"\"\n",
    "\n",
    "    return client.models.generate_content(model=MODEL, contents=[e1_prompt]).text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (20,10) (10,10) ",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 10\u001B[39m\n\u001B[32m      7\u001B[39m     verifier = OptimizerVerifier()\n\u001B[32m      8\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m verifier.verify(metaheuristic)\n\u001B[32m---> \u001B[39m\u001B[32m10\u001B[39m \u001B[38;5;28;43msorted\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mbase_metaheuristics\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mkey\u001B[49m\u001B[43m=\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[18]\u001B[39m\u001B[32m, line 8\u001B[39m, in \u001B[36mf\u001B[39m\u001B[34m(text)\u001B[39m\n\u001B[32m      6\u001B[39m metaheuristic = FunctionParser.parse(text, \u001B[33m\"\u001B[39m\u001B[33mnew_metaheuristic\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m      7\u001B[39m verifier = OptimizerVerifier()\n\u001B[32m----> \u001B[39m\u001B[32m8\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mverifier\u001B[49m\u001B[43m.\u001B[49m\u001B[43mverify\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmetaheuristic\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 34\u001B[39m, in \u001B[36mOptimizerVerifier.verify\u001B[39m\u001B[34m(self, optimizer)\u001B[39m\n\u001B[32m     32\u001B[39m     np.random.seed(seed)\n\u001B[32m     33\u001B[39m     random.seed(seed)\n\u001B[32m---> \u001B[39m\u001B[32m34\u001B[39m     best_val, _ = \u001B[43moptimizer\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mtest_function\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbudget\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     35\u001B[39m     results.append(best_val)\n\u001B[32m     36\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m np.mean(best_val)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m<string>:55\u001B[39m, in \u001B[36mnew_metaheuristic\u001B[39m\u001B[34m(function, bounds, budget)\u001B[39m\n",
      "\u001B[31mValueError\u001B[39m: operands could not be broadcast together with shapes (20,10) (10,10) "
     ]
    }
   ],
   "source": [
    "base_metaheuristics = [\n",
    "    client.models.generate_content(model=MODEL, contents=[PROMPT_METAHEURISTIC]).text\n",
    "    for _ in range(25)\n",
    "]\n",
    "def f(text):\n",
    "    metaheuristic = FunctionParser.parse(text, \"new_metaheuristic\")\n",
    "    verifier = OptimizerVerifier()\n",
    "    return verifier.verify(metaheuristic)\n",
    "\n",
    "sorted(base_metaheuristics, key=f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T09:12:36.398986Z",
     "start_time": "2025-05-27T09:09:45.723459Z"
    }
   },
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "25"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(base_metaheuristics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T09:13:07.371296Z",
     "start_time": "2025-05-27T09:13:07.365423Z"
    }
   },
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "40.98108303588725"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m1_metaheuristic = FunctionParser.parse(m1_response.text, \"new_metaheuristic\")\n",
    "\n",
    "verifier.verify(m1_metaheuristic)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-05-27T09:00:29.098462Z",
     "start_time": "2025-05-27T09:00:26.067847Z"
    }
   },
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run one of these operators, and try to explain what happened? Are the new mutated solution better than previous one? Calculate the quality of new solution and old ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3:\n",
    "Implement Evolution of Heuristics (or at least part of it). Start simple (pseudocode):\n",
    "```\n",
    "current_population = [generate_heuristic()]\n",
    "for _ in range(10):\n",
    "   parents = current_population[-p:]\n",
    "   new_solution = E1(parents) # Read 3.4. Prompt Strategies to better understand E1\n",
    "   f_new_solution = verify(new_solution)\n",
    "   current_population.append(new_solution)\n",
    "```\n",
    "Save all solutions (best would be to have a separate file for each one). Analyse 5 different ones. Plot quality of solution per epoch. Is this iterative process converging?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to Action\n",
    "The GECCO 2025 conference is hosting a [competition](https://gecco-2025.sigevo.org/Competition?itemId=5104) on LLM-designed Evolutionary Algorithms. If you’re interested in collaborating and participating as a team, feel free to send me a direct message. Let’s explore the opportunity together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Recommended Reading\n",
    "- [AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms](https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/)\n",
    "- Shojaee, Parshin, et al. [LLM-SR: Scientific equation discovery via programming with large language models.](https://arxiv.org/abs/2404.18400)\n",
    "- Romera-Paredes, Bernardino, et al. [Mathematical discoveries from program search with large language models.](https://www.nature.com/articles/s41586-023-06924-6)\n",
    "- van Stein, Niki, and Thomas Bäck. [Llamea: A large language model evolutionary algorithm for automatically generating metaheuristics.](https://arxiv.org/abs/2405.20132)\n",
    "- Liu, Fei, et al. [Evolution of heuristics: Towards efficient automatic algorithm design using large language model.](https://arxiv.org/abs/2401.02051)\n",
    "- van Stein, Niki, et al. [BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics](https://arxiv.org/html/2504.20183v1)\n",
    "- [OpenEvolve](https://github.com/codelion/openevolve)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
